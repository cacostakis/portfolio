[
  {
    "objectID": "2025_12_23_world_languages/world_languages.html",
    "href": "2025_12_23_world_languages/world_languages.html",
    "title": "African Language Diversity",
    "section": "",
    "text": "African Languages\n\nWeek of December 23, 2025\nThe presence of African languages is evidence of a rich ethnic history, multiple intersecting language families, sociopolitical factors, and globalization, which has endangered many previously spoken languages in favor of those with larger population bases.\n\nData Source: Glottolog 5.2, Dec 2025"
  },
  {
    "objectID": "2025_11_25_spi_index/spi_indicators_regression.html",
    "href": "2025_11_25_spi_index/spi_indicators_regression.html",
    "title": "spi_indicators_regression",
    "section": "",
    "text": "Data Development Indicators\n\nWeek of 11 Nov 2025\nThe SPI index is a globally tracked set of indicators which score a country’s data use, data services, data products, data sources, and data infrastructure. These 5 areas of data development are combined to calculate an overall score, representing the level of data development achieved by the country. The index is calculated on a yearly basis.\nI used the tidytuesdayR package in R to download the dataset and conduct exploratory data analysis.\n\nlibrary(tidyverse)\nlibrary(modelr) \nlibrary(psych)\nlibrary(tidytuesdayR)\nlibrary(broom)\nlibrary(ggiraph)\n\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-11-25')\nspi_indicators &lt;- tuesdata$spi_indicators\n\n\nWhich SPI indicators best predict overall score?\nI was curious about which indicators are most predictive of the overall score. For example, if the measurement of ‘data use’ were extremely noisy or inconsistent with on-the-ground realities, it is likely that it would be a less predictive indicator of the overall score.\nI used ‘Forward Selection’, by running linear regressions of the overall score across each indicator individually in order to identify the strongest predictor. This is displayed below in the final table, with data infrastructure being the strongest predictor of overall score.\n\nindicators &lt;- c(\"data_use_score\", \"data_services_score\", \"data_products_score\", \n                \"data_sources_score\", \"data_infrastructure_score\")\n\nsingle_indicators &lt;- map(indicators, ~lm(reformulate(.x, \"overall_score\"), data = spi_indicators))\n\nmap_dfr(single_indicators, glance, .id = \"predictor\") %&gt;% arrange(desc(r.squared)) %&gt;%\n  mutate(predictor = factor(predictor, levels = 1:5, labels = str_remove(indicators, \"_score\")))\n\n# A tibble: 5 × 13\n  predictor       r.squared adj.r.squared sigma statistic   p.value    df logLik\n  &lt;fct&gt;               &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 data_infrastru…     0.788         0.788  8.07     5287. 0             1 -4996.\n2 data_sources        0.771         0.771  8.38     4796. 0             1 -5050.\n3 data_services       0.742         0.741  8.90     4085. 0             1 -5137.\n4 data_use            0.673         0.673 10.0      2926. 0             1 -5305.\n5 data_products       0.493         0.493 12.5      1385. 2.77e-212     1 -5617.\n# ℹ 5 more variables: AIC &lt;dbl&gt;, BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt;,\n#   nobs &lt;int&gt;\n\n\nNext I conducted another round of regressions, adding the remaining indicators to the data infrastructure indicator. The final output table shows that the data services and data infrastructure indicators account for 91% of the variation in the overall score.\n\nadded_indicators_to_infrastructure &lt;- map(setdiff(indicators, \"data_infrastructure_score\"), ~lm(reformulate(termlabels = c(.x, \"data_infrastructure_score\"), \"overall_score\"), data = spi_indicators))\n\nmap_dfr(added_indicators_to_infrastructure, glance, .id = \"predictor\") %&gt;% arrange(desc(r.squared)) %&gt;%\n  mutate(predictor = factor(predictor, levels = 1:5, labels = str_replace(indicators, \"_score\", \" + infrastructure\")))\n\n# A tibble: 4 × 13\n  predictor   r.squared adj.r.squared sigma statistic p.value    df logLik   AIC\n  &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 data_servi…     0.918         0.918  5.02     7951.       0     2 -4320. 8648.\n2 data_use +…     0.896         0.896  5.64     6148.       0     2 -4486. 8980.\n3 data_sourc…     0.887         0.887  5.88     5606.       0     2 -4545. 9097.\n4 data_produ…     0.873         0.873  6.25     4886.       0     2 -4631. 9270.\n# ℹ 4 more variables: BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n\nVisualizing the model’s accuracy\nAt this point, I felt this was a reasonably high amount of variation to have accounted for. Instead of adding more indicators, I added an interaction variable to check if the interaction between infrastructure and services improves the model. The final output table demonstrates that is does not improve the model, with the R-squared dropping into the 80s.\n\nlm_inf_serv_interaction &lt;- lm(overall_score ~ data_services_score:data_infrastructure_score, data = spi_indicators)\n\nglance(lm_inf_serv_interaction) %&gt;% mutate(predictor = \"Data Infrastructure : Services\") %&gt;% relocate(predictor)\n\n# A tibble: 1 × 13\n  predictor   r.squared adj.r.squared sigma statistic p.value    df logLik   AIC\n  &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 Data Infra…     0.857         0.857  6.62     8553.       0     1 -4713. 9433.\n# ℹ 4 more variables: BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nIn order to visualize the relationship, I made a new df with the added predictions and residuals, where a positive residual means that the real-world observed overall score outperformed the predicted score for the country.\nA few choices in this process include: - Removing years before 2016, for which there was no data for three of the indicators - Recoding Venezuela as a lower-middle income country (it was previously unclassified in 2021 due to a lack of reporting data) - Averaging each country’s yearly residuals (between the predicted and observed overall score) to calculate a single average residual\n\nspi_model&lt;- \nspi_indicators %&gt;%\n  add_predictions(added_indicators_to_infrastructure[[2]]) %&gt;%                  # Add predictions and residuals\n  add_residuals(added_indicators_to_infrastructure[[2]]) %&gt;%\n  filter(year &gt;= 2016)                              # Remove years without full indicator data\n\nspi_model_summary &lt;-\n  spi_model %&gt;%\n  group_by(country) %&gt;%\n  summarize(\n    income = first(income),\n    region = first(region),\n    mean_resid = mean(resid, na.rm = T),\n    mean_abs_resid = mean(abs(resid), na.rm = T),\n    n_years = sum(!is.na(resid)),                   # How many years of data\n    sse = sum(resid^2, na.rm = T),                  # Sum of squared errors\n    mse = mean(resid^2, na.rm = T)                  # Mean squared error\n  ) %&gt;%\n  filter(!is.na(mean_resid)) %&gt;%                    # Remove countries with NA residuals\n  mutate(\n    income = if_else(country==\"Venezuela, RB\",      # Recode Venezuela\n                     \"Lower middle income\", \n                     income))\n\nThe plot below shows countries relative to their predicted Data Development score. It is noticeable that the Upper Middle income countries are largely outperforming their model prediction, whereas high and low income countries are under performing their prediction. This is most noticeable among low income countries.\n\nspi_resid_sd &lt;- spi_model_summary$mean_resid %&gt;% sd # Calculate 1 standard deviation of residuals\n  \nggplot(data = spi_model_summary,\n         aes(x = mean_resid, y = fct_reorder(country, mean_resid))) +\n  geom_vline(xintercept = 0, color = \"lightblue\", size = 1) + \n  geom_vline(xintercept = 0+spi_resid_sd, color = \"lightgreen\", size = 0.8) + \n  geom_vline(xintercept = 0-spi_resid_sd, color = \"salmon\", size = 0.8) + \n  geom_point() +\n  facet_wrap(~income, scales =\"free_y\") +\n  labs(title = \"Average residual between the observed Data Development score and the model's predicted score\",\n       subtitle = \"The model predict Data Development score by using the Data infrastructure and Services indicators only \\nPositive residuals indicates that real-world scores were higher than the predicted model.\",\n       x = \"Average Residual between Real-world performance and model\") +\n  theme(axis.title.y = element_blank(),\n        axis.text.y = element_text(size = 3)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nWhat indicators improve on the original model for low income countries?\nThis prompted a second question. Can low income countries be better modeled using different indicators? There may be other factors in low income countries that makes one indicator or another more representative of its overall Data Development score. For example, maybe a lack of infrastructure development in low income countries made this indicator less predictive of overall development.\nI decided to answer this be conducting the same process as before. I filtered the data set to only include low income countries, and then used Forward Selection to arrive at the most predictive indicators.\nThe output table below demonstrates that, in fact, data products are the most predictive single indicator. Products were previously the weakest indicator, and refers to the actual statistics which are available for the country (centered on statistics pertaining to Sustainable Development Goals).\n\nspi_low &lt;-\n  spi_indicators %&gt;%\n  filter(income == \"Low income\")\n\nsingle_low_income_indicators &lt;- map(indicators, ~lm(reformulate(.x, \"overall_score\"), data = spi_low))\n\nmap_dfr(single_low_income_indicators, glance, .id = \"predictor\") %&gt;% arrange(desc(r.squared)) %&gt;%\n  mutate(predictor = factor(predictor, levels = 1:5, labels = str_remove(indicators, \"_score\")))\n\n# A tibble: 5 × 13\n  predictor  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC\n  &lt;fct&gt;          &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 data_prod…     0.763         0.762  6.20      583. 1.73e-58     1  -593. 1191.\n2 data_use       0.702         0.701  6.95      427. 1.74e-49     1  -613. 1233.\n3 data_sour…     0.630         0.628  7.75      308. 6.52e-41     1  -633. 1273.\n4 data_serv…     0.612         0.609  7.94      285. 5.19e-39     1  -638. 1281.\n5 data_infr…     0.600         0.598  8.05      272. 6.67e-38     1  -640. 1287.\n# ℹ 4 more variables: BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nBoth Data Use and Data Sources were shown to be similarly effective secondary predictors, when added to the primary Data Products indicator. This model accounted for roughly 88% percent of the variation in the Data Development score. The interaction variable did not improve the accuracy of either model.\n\nadded_low_income_indicators_to_infrastructure &lt;- map(setdiff(indicators, \"data_products_score\"), ~lm(reformulate(termlabels = c(.x, \"data_products_score\"), \"overall_score\"), data = spi_low))\n\nmap_dfr(added_low_income_indicators_to_infrastructure, glance, .id = \"predictor\") %&gt;% arrange(desc(r.squared)) %&gt;%\n  mutate(predictor = factor(predictor, \n                            levels = 1:4, \n                            labels = str_replace(setdiff(indicators, \"data_products_score\"), \"_score\", \" + products\")))\n\n# A tibble: 4 × 13\n  predictor  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC\n  &lt;fct&gt;          &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 data_sour…     0.880         0.878  4.43      658. 1.67e-83     2  -530. 1069.\n2 data_use …     0.874         0.872  4.54      623. 1.22e-81     2  -535. 1078.\n3 data_serv…     0.833         0.832  5.21      450. 9.03e-71     2  -560. 1129.\n4 data_infr…     0.829         0.827  5.29      435. 1.14e-69     2  -563. 1134.\n# ℹ 4 more variables: BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nlm_low_income_interaction &lt;- lm(overall_score ~ data_products_score:data_sources_score, data = spi_low)\n\nglance(lm_low_income_interaction) %&gt;% mutate(predictor = \"Data Products:Data Sources interaction model\") %&gt;% relocate(predictor)\n\n# A tibble: 1 × 13\n  predictor  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC\n  &lt;chr&gt;          &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 Data Prod…     0.762         0.761  6.21      581. 2.18e-58     1  -593. 1191.\n# ℹ 4 more variables: BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n\nVisualizing the Accuracy of the new Low Income-trained model\nBased on other checks of the Data Use vs Data Sources models (such as running a test of internal consistency using Cronbach’s alpha), I decided that Data Sources seems to be the better model. I present the same plot as above, but applying the ‘low income’ model to all countries. While this improves the fit for low income countries, it is a very poor prediction for all other income groups, predicting far below what those groups scored in the real-world.\n\nspi_model_prod_sourc_low &lt;-\n  spi_indicators %&gt;%\n  add_predictions(added_low_income_indicators_to_infrastructure[[2]]) %&gt;%\n  add_residuals(added_low_income_indicators_to_infrastructure[[2]]) %&gt;%\n  group_by(country) %&gt;%\n  summarize(\n    income = first(income),\n    region = first(region),\n    mean_resid = mean(resid, na.rm = T),\n    mean_abs_resid = mean(abs(resid), na.rm = T),  # Better measure of \"worse\"\n    n_years = sum(!is.na(resid)),                   # How many years of data\n    sse = sum(resid^2, na.rm = T),                  # Sum of squared errors\n    mse = mean(resid^2, na.rm = T)                  # Mean squared error\n  ) %&gt;%\n  filter(!is.na(mean_resid)) %&gt;%\n  mutate(income = if_else(country==\"Venezuela, RB\", \"Lower middle income\", income))\n\nspi_resid_sd_prod_sourc_low &lt;- spi_model_prod_sourc_low$mean_resid %&gt;% sd\n\nggplot(data = spi_model_prod_sourc_low,\n       aes(x = mean_resid, y = fct_reorder(country, mean_resid))) +\n  geom_vline(xintercept = 0, color = \"lightblue\", size = 1) + \n  geom_vline(xintercept = 0+spi_resid_sd_prod_sourc_low, color = \"lightgreen\", size = 0.8) + \n  geom_vline(xintercept = 0-spi_resid_sd_prod_sourc_low, color = \"salmon\", size = 0.8) + \n  geom_point() +\n  facet_wrap(~income, scales =\"free_y\")+\n  labs(title = \"Low income variation: Average residual between the observed Data Development score and the model's predicted score\",\n       subtitle = \"The model predicts Data Development score by using the Data Products and Sources indicators only, and was trained on low income country data only. \\nPositive residuals indicates that real-world scores were higher than the predicted model.\",\n       x = \"Average Residual between Real-world performance and model\") +\n  theme(axis.title.y = element_blank(),\n        axis.text.y = element_text(size = 3)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nMaking sense of the two models\nWhat do the two models demonstrate about Data Development?\nThe original model shows that, for most countries, we can reasonably predict overall Data Development scores through just the Data Infrastructure and Services indicators. Infrastructure includes hard and soft data systems and data governance, whereas services are tools that allow accessibility to a range of data platforms. Together, they represent openness, structure, sophistication of the data environment.\nThe residuals show that the income level of a country changes the predictiveness of those two indicators of overall Data Development. - High income countries under performed their prediction as a group - Middle income countries over performed their prediction as a group\nThis may be explained simply due to ceiling effects for high income countries. As a country nears the maximum score, small gaps can reduce scores, and achieving 100% may become increasingly difficult. For middle income countries with lots of room for improvement, growth is more readily obtained. This may have led to more over performance relative to the model, and suggesting that a non-linear model may be better suited.\nOn the other hand, Data Infrastructure and Services indicators generally over estimate the Data Development score of low income countries. Despite governance and accessibility features, other aspects of the data environment seem to result in lower overall scores. For such countries, Data Products (the actual statistics available) and Data Sources (whether data collection mechanisms exist) are the more important predictors. Whether or not more sophisticated structure exists is less important here. Instead, we should look at the hard data and tools available in the country, as this is a better indicator of overall Data Development."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m [Your Name], and I use #TidyTuesday as a way to practice data visualization and analysis skills.\nThis portfolio showcases my weekly explorations of diverse datasets, from simple insights to complex analytical deep-dives.\nTools I use: R, ggplot2, dplyr, Shiny\nConnect with me: - GitHub: your-github"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TidyTuesday Data Explorations",
    "section": "",
    "text": "Welcome to my weekly data visualization journey with #TidyTuesdayR\nI’m a former primary and middle school teacher. Early grade reading and teaching English Language Learners was my most common focus. I’ve spent time in US public and tribal schools, and taught in a rural West African school as a Peace Corps volunteer.\nI now work in education evaluation and research. My core job is in working with implementing organizations to better use data and create more effective education programs around the world. That means that I often don’t get to practice my hard analysis skills, since I’m busy doing other things, like capacity-building with program staff, research design, instrument development, data collection quality controls, and writing up or disseminating findings. (Look up my cool org if you interested in more.)\n\nI’ve started this page to help build my skills in getting my analysis into sharable format, and provide self-motivation to keep up with the weekly TidyTuesdayR community.\nFeel free to browse through my weekly contributions.\n\n\nTidyTuesday Contributions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeaths in Gaza Since 7 October, 2023\n\n\n\nplotting\n\n\n\n\n\n\n\n\n\nJan 15, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPredictive power of Data Development Indicators\n\n\n\npredictive modelling\n\n\n\n\n\n\n\n\n\nJan 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPredictive power of Data Development Indicators\n\n\n\npredictive modelling\n\n\n\n\n\n\n\n\n\nJan 11, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "All Posts",
    "section": "",
    "text": "TidyTuesdayR Contributions\n\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nDeaths in Gaza Since 7 October, 2023\n\n\n\nplotting\n\n\n\n\n\n\n\n\n\nJan 15, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPredictive power of Data Development Indicators\n\n\n\npredictive modelling\n\n\n\n\n\n\n\n\n\nJan 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPredictive power of Data Development Indicators\n\n\n\npredictive modelling\n\n\n\n\n\n\n\n\n\nJan 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAfrican Language Diversity\n\n\n\nmapping\n\n\n\nLanguages have evolved dynamically across the African continent\n\n\n\n\n\nJan 10, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025_12_23_world_languages/index.html",
    "href": "posts/2025_12_23_world_languages/index.html",
    "title": "African Language Diversity",
    "section": "",
    "text": "African Languages\n\nWeek of December 23, 2025\nThe presence of African languages is evidence of a rich ethnic history, multiple intersecting language families, sociopolitical factors, and globalization, which has endangered many previously spoken languages in favor of those with larger population bases.\n\nData Source: Glottolog 5.2, Dec 2025"
  },
  {
    "objectID": "posts/2025_11_25_spi_index/index.html",
    "href": "posts/2025_11_25_spi_index/index.html",
    "title": "Predictive power of Data Development Indicators",
    "section": "",
    "text": "Data Development Indicators\n\nWeek of 11 Nov 2025\nThe SPI index is a globally tracked set of indicators which score a country’s data use, data services, data products, data sources, and data infrastructure. These 5 areas of data development are combined to calculate an overall score, representing the level of data development achieved by the country. The index is calculated on a yearly basis.\nData source: World Bank Group - Statistical Performance Indicators\nI used the tidytuesdayR package in R to download the dataset and conduct exploratory data analysis.\n\nlibrary(tidyverse)\nlibrary(modelr) \nlibrary(psych)\nlibrary(tidytuesdayR)\nlibrary(broom)\nlibrary(ggiraph)\n\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-11-25')\nspi_indicators &lt;- tuesdata$spi_indicators\n\n\nWhich SPI indicators best predict overall score?\nI was curious about which indicators are most predictive of the overall score. For example, if the measurement of ‘data use’ were extremely noisy or inconsistent with on-the-ground realities, it is likely that it would be a less predictive indicator of the overall score.\nI used ‘Forward Selection’, by running linear regressions of the overall score across each indicator individually in order to identify the strongest predictor. This is displayed below in the final table, with data infrastructure being the strongest predictor of overall score.\n\nindicators &lt;- c(\"data_use_score\", \"data_services_score\", \"data_products_score\", \n                \"data_sources_score\", \"data_infrastructure_score\")\n\nsingle_indicators &lt;- map(indicators, ~lm(reformulate(.x, \"overall_score\"), data = spi_indicators))\n\nmap_dfr(single_indicators, glance, .id = \"predictor\") %&gt;% arrange(desc(r.squared)) %&gt;%\n  mutate(predictor = factor(predictor, levels = 1:5, labels = str_remove(indicators, \"_score\")))\n\n# A tibble: 5 × 13\n  predictor       r.squared adj.r.squared sigma statistic   p.value    df logLik\n  &lt;fct&gt;               &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 data_infrastru…     0.788         0.788  8.07     5287. 0             1 -4996.\n2 data_sources        0.771         0.771  8.38     4796. 0             1 -5050.\n3 data_services       0.742         0.741  8.90     4085. 0             1 -5137.\n4 data_use            0.673         0.673 10.0      2926. 0             1 -5305.\n5 data_products       0.493         0.493 12.5      1385. 2.77e-212     1 -5617.\n# ℹ 5 more variables: AIC &lt;dbl&gt;, BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt;,\n#   nobs &lt;int&gt;\n\n\nNext I conducted another round of regressions, adding the remaining indicators to the data infrastructure indicator. The final output table shows that the data services and data infrastructure indicators account for 91% of the variation in the overall score.\n\nadded_indicators_to_infrastructure &lt;- map(setdiff(indicators, \"data_infrastructure_score\"), ~lm(reformulate(termlabels = c(.x, \"data_infrastructure_score\"), \"overall_score\"), data = spi_indicators))\n\nmap_dfr(added_indicators_to_infrastructure, glance, .id = \"predictor\") %&gt;% arrange(desc(r.squared)) %&gt;%\n  mutate(predictor = factor(predictor, levels = 1:5, labels = str_replace(indicators, \"_score\", \" + infrastructure\")))\n\n# A tibble: 4 × 13\n  predictor   r.squared adj.r.squared sigma statistic p.value    df logLik   AIC\n  &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 data_servi…     0.918         0.918  5.02     7951.       0     2 -4320. 8648.\n2 data_use +…     0.896         0.896  5.64     6148.       0     2 -4486. 8980.\n3 data_sourc…     0.887         0.887  5.88     5606.       0     2 -4545. 9097.\n4 data_produ…     0.873         0.873  6.25     4886.       0     2 -4631. 9270.\n# ℹ 4 more variables: BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n\nVisualizing the model’s accuracy\nAt this point, I felt this was a reasonably high amount of variation to have accounted for. Instead of adding more indicators, I added an interaction variable to check if the interaction between infrastructure and services improves the model. The final output table demonstrates that is does not improve the model, with the R-squared dropping into the 80s.\n\nlm_inf_serv_interaction &lt;- lm(overall_score ~ data_services_score:data_infrastructure_score, data = spi_indicators)\n\nglance(lm_inf_serv_interaction) %&gt;% mutate(predictor = \"Data Infrastructure : Services\") %&gt;% relocate(predictor)\n\n# A tibble: 1 × 13\n  predictor   r.squared adj.r.squared sigma statistic p.value    df logLik   AIC\n  &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 Data Infra…     0.857         0.857  6.62     8553.       0     1 -4713. 9433.\n# ℹ 4 more variables: BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nIn order to visualize the relationship, I made a new df with the added predictions and residuals, where a positive residual means that the real-world observed overall score outperformed the predicted score for the country.\nA few choices in this process include: - Removing years before 2016, for which there was no data for three of the indicators - Recoding Venezuela as a lower-middle income country (it was previously unclassified in 2021 due to a lack of reporting data) - Averaging each country’s yearly residuals (between the predicted and observed overall score) to calculate a single average residual\n\nspi_model&lt;- \nspi_indicators %&gt;%\n  add_predictions(added_indicators_to_infrastructure[[2]]) %&gt;%                  # Add predictions and residuals\n  add_residuals(added_indicators_to_infrastructure[[2]]) %&gt;%\n  filter(year &gt;= 2016)                              # Remove years without full indicator data\n\nspi_model_summary &lt;-\n  spi_model %&gt;%\n  group_by(country) %&gt;%\n  summarize(\n    income = first(income),\n    region = first(region),\n    mean_resid = mean(resid, na.rm = T),\n    mean_abs_resid = mean(abs(resid), na.rm = T),\n    n_years = sum(!is.na(resid)),                   # How many years of data\n    sse = sum(resid^2, na.rm = T),                  # Sum of squared errors\n    mse = mean(resid^2, na.rm = T)                  # Mean squared error\n  ) %&gt;%\n  filter(!is.na(mean_resid)) %&gt;%                    # Remove countries with NA residuals\n  mutate(\n    income = if_else(country==\"Venezuela, RB\",      # Recode Venezuela\n                     \"Lower middle income\", \n                     income))\n\nThe plot below shows countries relative to their predicted Data Development score. It is noticeable that the Upper Middle income countries are largely outperforming their model prediction, whereas high and low income countries are under performing their prediction. This is most noticeable among low income countries.\n\nspi_resid_sd &lt;- spi_model_summary$mean_resid %&gt;% sd # Calculate 1 standard deviation of residuals\n  \nggplot(data = spi_model_summary,\n         aes(x = mean_resid, y = fct_reorder(country, mean_resid))) +\n  geom_vline(xintercept = 0, color = \"lightblue\", size = 1) + \n  geom_vline(xintercept = 0+spi_resid_sd, color = \"lightgreen\", size = 0.8) + \n  geom_vline(xintercept = 0-spi_resid_sd, color = \"salmon\", size = 0.8) + \n  geom_point() +\n  facet_wrap(~income, scales =\"free_y\") +\n  labs(title = \"Average residual between the observed Data Development score and the model's predicted score\",\n       subtitle = \"The model predict Data Development score by using the Data infrastructure and Services indicators only \\nPositive residuals indicates that real-world scores were higher than the predicted model.\",\n       x = \"Average Residual between Real-world performance and model\") +\n  theme(axis.title.y = element_blank(),\n        axis.text.y = element_text(size = 3)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nWhat indicators improve on the original model for low income countries?\nThis prompted a second question. Can low income countries be better modeled using different indicators? There may be other factors in low income countries that makes one indicator or another more representative of its overall Data Development score. For example, maybe a lack of infrastructure development in low income countries made this indicator less predictive of overall development.\nI decided to answer this be conducting the same process as before. I filtered the data set to only include low income countries, and then used Forward Selection to arrive at the most predictive indicators.\nThe output table below demonstrates that, in fact, data products are the most predictive single indicator. Products were previously the weakest indicator, and refers to the actual statistics which are available for the country (centered on statistics pertaining to Sustainable Development Goals).\n\nspi_low &lt;-\n  spi_indicators %&gt;%\n  filter(income == \"Low income\")\n\nsingle_low_income_indicators &lt;- map(indicators, ~lm(reformulate(.x, \"overall_score\"), data = spi_low))\n\nmap_dfr(single_low_income_indicators, glance, .id = \"predictor\") %&gt;% arrange(desc(r.squared)) %&gt;%\n  mutate(predictor = factor(predictor, levels = 1:5, labels = str_remove(indicators, \"_score\")))\n\n# A tibble: 5 × 13\n  predictor  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC\n  &lt;fct&gt;          &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 data_prod…     0.763         0.762  6.20      583. 1.73e-58     1  -593. 1191.\n2 data_use       0.702         0.701  6.95      427. 1.74e-49     1  -613. 1233.\n3 data_sour…     0.630         0.628  7.75      308. 6.52e-41     1  -633. 1273.\n4 data_serv…     0.612         0.609  7.94      285. 5.19e-39     1  -638. 1281.\n5 data_infr…     0.600         0.598  8.05      272. 6.67e-38     1  -640. 1287.\n# ℹ 4 more variables: BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nBoth Data Use and Data Sources were shown to be similarly effective secondary predictors, when added to the primary Data Products indicator. This model accounted for roughly 88% percent of the variation in the Data Development score. The interaction variable did not improve the accuracy of either model.\n\nadded_low_income_indicators_to_infrastructure &lt;- map(setdiff(indicators, \"data_products_score\"), ~lm(reformulate(termlabels = c(.x, \"data_products_score\"), \"overall_score\"), data = spi_low))\n\nmap_dfr(added_low_income_indicators_to_infrastructure, glance, .id = \"predictor\") %&gt;% arrange(desc(r.squared)) %&gt;%\n  mutate(predictor = factor(predictor, \n                            levels = 1:4, \n                            labels = str_replace(setdiff(indicators, \"data_products_score\"), \"_score\", \" + products\")))\n\n# A tibble: 4 × 13\n  predictor  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC\n  &lt;fct&gt;          &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 data_sour…     0.880         0.878  4.43      658. 1.67e-83     2  -530. 1069.\n2 data_use …     0.874         0.872  4.54      623. 1.22e-81     2  -535. 1078.\n3 data_serv…     0.833         0.832  5.21      450. 9.03e-71     2  -560. 1129.\n4 data_infr…     0.829         0.827  5.29      435. 1.14e-69     2  -563. 1134.\n# ℹ 4 more variables: BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nlm_low_income_interaction &lt;- lm(overall_score ~ data_products_score:data_sources_score, data = spi_low)\n\nglance(lm_low_income_interaction) %&gt;% mutate(predictor = \"Data Products:Data Sources interaction model\") %&gt;% relocate(predictor)\n\n# A tibble: 1 × 13\n  predictor  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC\n  &lt;chr&gt;          &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 Data Prod…     0.762         0.761  6.21      581. 2.18e-58     1  -593. 1191.\n# ℹ 4 more variables: BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n\nVisualizing the Accuracy of the new Low Income-trained model\nBased on other checks of the Data Use vs Data Sources models (such as running a test of internal consistency using Cronbach’s alpha), I decided that Data Sources seems to be the better model. I present the same plot as above, but applying the ‘low income’ model to all countries. While this improves the fit for low income countries, it is a very poor prediction for all other income groups, predicting far below what those groups scored in the real-world.\n\nspi_model_prod_sourc_low &lt;-\n  spi_indicators %&gt;%\n  add_predictions(added_low_income_indicators_to_infrastructure[[2]]) %&gt;%\n  add_residuals(added_low_income_indicators_to_infrastructure[[2]]) %&gt;%\n  group_by(country) %&gt;%\n  summarize(\n    income = first(income),\n    region = first(region),\n    mean_resid = mean(resid, na.rm = T),\n    mean_abs_resid = mean(abs(resid), na.rm = T),  # Better measure of \"worse\"\n    n_years = sum(!is.na(resid)),                   # How many years of data\n    sse = sum(resid^2, na.rm = T),                  # Sum of squared errors\n    mse = mean(resid^2, na.rm = T)                  # Mean squared error\n  ) %&gt;%\n  filter(!is.na(mean_resid)) %&gt;%\n  mutate(income = if_else(country==\"Venezuela, RB\", \"Lower middle income\", income))\n\nspi_resid_sd_prod_sourc_low &lt;- spi_model_prod_sourc_low$mean_resid %&gt;% sd\n\nggplot(data = spi_model_prod_sourc_low,\n       aes(x = mean_resid, y = fct_reorder(country, mean_resid))) +\n  geom_vline(xintercept = 0, color = \"lightblue\", size = 1) + \n  geom_vline(xintercept = 0+spi_resid_sd_prod_sourc_low, color = \"lightgreen\", size = 0.8) + \n  geom_vline(xintercept = 0-spi_resid_sd_prod_sourc_low, color = \"salmon\", size = 0.8) + \n  geom_point() +\n  facet_wrap(~income, scales =\"free_y\")+\n  labs(title = \"Low income variation: Average residual between the observed Data Development score and the model's predicted score\",\n       subtitle = \"The model predicts Data Development score by using the Data Products and Sources indicators only, and was trained on low income country data only. \\nPositive residuals indicates that real-world scores were higher than the predicted model.\",\n       x = \"Average Residual between Real-world performance and model\") +\n  theme(axis.title.y = element_blank(),\n        axis.text.y = element_text(size = 3)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nMaking sense of the two models\nWhat do the two models demonstrate about Data Development?\nThe original model shows that, for most countries, we can reasonably predict overall Data Development scores through just the Data Infrastructure and Services indicators. Infrastructure includes hard and soft data systems and data governance, whereas services are tools that allow accessibility to a range of data platforms. Together, they represent openness, structure, sophistication of the data environment.\nThe residuals show that the income level of a country changes the predictiveness of those two indicators of overall Data Development. - High income countries under performed their prediction as a group - Middle income countries over performed their prediction as a group\nThis may be explained simply due to ceiling effects for high income countries. As a country nears the maximum score, small gaps can reduce scores, and achieving 100% may become increasingly difficult. For middle income countries with lots of room for improvement, growth is more readily obtained. This may have led to more over performance relative to the model, and suggesting that a non-linear model may be better suited.\nOn the other hand, Data Infrastructure and Services indicators generally over estimate the Data Development score of low income countries. Despite governance and accessibility features, other aspects of the data environment seem to result in lower overall scores. For such countries, Data Products (the actual statistics available) and Data Sources (whether data collection mechanisms exist) are the more important predictors. Whether or not more sophisticated structure exists is less important here. Instead, we should look at the hard data and tools available in the country, as this is a better indicator of overall Data Development."
  },
  {
    "objectID": "posts/palestine_deaths/index.html",
    "href": "posts/palestine_deaths/index.html",
    "title": "Deaths in Gaza Since 7 October, 2023",
    "section": "",
    "text": "Gaza Deaths since 7 October, 2023\n\nNot TidyTuesday data\nAs someone who only hears about Palestine from afar, I wanted to know more about the size of impact on humans. I will continue to add more visuals.\n\n\n\nData source: Tech for Palestine Indicators"
  }
]