{
  "hash": "1caefa93efb9199386aa18e93d94409e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"spi_indicators_regression\"\nformat: html\nwarning: false\nmessage: false\n---\n\n# Data Development Indicators\n\n### Week of 11 Nov 2025\n\nThe SPI index is a globally tracked set of indicators which score a country's data use, data services, data products, data sources, and data infrastructure. These 5 areas of data development are combined to calculate an overall score, representing the level of data development achieved by the country. The index is calculated on a yearly basis. \n\nI used the tidytuesdayR package in R to download the dataset and conduct exploratory data analysis. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(modelr) \nlibrary(psych)\nlibrary(tidytuesdayR)\nlibrary(broom)\nlibrary(ggiraph)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntuesdata <- tidytuesdayR::tt_load('2025-11-25')\nspi_indicators <- tuesdata$spi_indicators\n```\n:::\n\n\n#### Which SPI indicators best predict overall score?\n\nI was curious about which indicators are most predictive of the overall score. For example, if the measurement of 'data use' were extremely noisy or inconsistent with on-the-ground realities, it is likely that it would be a less predictive indicator of the overall score.\n\nI used 'Forward Selection', by running linear regressions of the overall score across each indicator *individually* in order to identify the strongest predictor. This is displayed below in the final table, with data infrastructure being the strongest predictor of overall score.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nindicators <- c(\"data_use_score\", \"data_services_score\", \"data_products_score\", \n                \"data_sources_score\", \"data_infrastructure_score\")\n\nsingle_indicators <- map(indicators, ~lm(reformulate(.x, \"overall_score\"), data = spi_indicators))\n\nmap_dfr(single_indicators, glance, .id = \"predictor\") %>% arrange(desc(r.squared)) %>%\n  mutate(predictor = factor(predictor, levels = 1:5, labels = str_remove(indicators, \"_score\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 13\n  predictor       r.squared adj.r.squared sigma statistic   p.value    df logLik\n  <fct>               <dbl>         <dbl> <dbl>     <dbl>     <dbl> <dbl>  <dbl>\n1 data_infrastru…     0.788         0.788  8.07     5287. 0             1 -4996.\n2 data_sources        0.771         0.771  8.38     4796. 0             1 -5050.\n3 data_services       0.742         0.741  8.90     4085. 0             1 -5137.\n4 data_use            0.673         0.673 10.0      2926. 0             1 -5305.\n5 data_products       0.493         0.493 12.5      1385. 2.77e-212     1 -5617.\n# ℹ 5 more variables: AIC <dbl>, BIC <dbl>, deviance <dbl>, df.residual <int>,\n#   nobs <int>\n```\n\n\n:::\n:::\n\n\nNext I conducted another round of regressions, adding the remaining indicators to the data infrastructure indicator. The final output table shows that the data services and data infrastructure indicators account for 91% of the variation in the overall score. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nadded_indicators_to_infrastructure <- map(setdiff(indicators, \"data_infrastructure_score\"), ~lm(reformulate(termlabels = c(.x, \"data_infrastructure_score\"), \"overall_score\"), data = spi_indicators))\n\nmap_dfr(added_indicators_to_infrastructure, glance, .id = \"predictor\") %>% arrange(desc(r.squared)) %>%\n  mutate(predictor = factor(predictor, levels = 1:5, labels = str_replace(indicators, \"_score\", \" + infrastructure\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 13\n  predictor   r.squared adj.r.squared sigma statistic p.value    df logLik   AIC\n  <fct>           <dbl>         <dbl> <dbl>     <dbl>   <dbl> <dbl>  <dbl> <dbl>\n1 data_servi…     0.918         0.918  5.02     7951.       0     2 -4320. 8648.\n2 data_use +…     0.896         0.896  5.64     6148.       0     2 -4486. 8980.\n3 data_sourc…     0.887         0.887  5.88     5606.       0     2 -4545. 9097.\n4 data_produ…     0.873         0.873  6.25     4886.       0     2 -4631. 9270.\n# ℹ 4 more variables: BIC <dbl>, deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n:::\n\n\n#### Visualizing the model's accuracy\n\nAt this point, I felt this was a reasonably high amount of variation to have accounted for. Instead of adding more indicators, I added an interaction variable to check if the interaction between infrastructure and services improves the model. The final output table demonstrates that is does *not improve* the model, with the R-squared dropping into the 80s. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_inf_serv_interaction <- lm(overall_score ~ data_services_score:data_infrastructure_score, data = spi_indicators)\n\nglance(lm_inf_serv_interaction) %>% mutate(predictor = \"Data Infrastructure : Services\") %>% relocate(predictor)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 13\n  predictor   r.squared adj.r.squared sigma statistic p.value    df logLik   AIC\n  <chr>           <dbl>         <dbl> <dbl>     <dbl>   <dbl> <dbl>  <dbl> <dbl>\n1 Data Infra…     0.857         0.857  6.62     8553.       0     1 -4713. 9433.\n# ℹ 4 more variables: BIC <dbl>, deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n:::\n\n\nIn order to visualize the relationship, I made a new df with the added predictions and residuals, where a positive residual means that the real-world observed overall score outperformed the predicted score for the country. \n\nA few choices in this process include:\n- Removing years before 2016, for which there was no data for three of the indicators\n- Recoding Venezuela as a lower-middle income country (it was previously unclassified in 2021 due to a lack of reporting data)\n- Averaging each country's yearly residuals (between the predicted and observed overall score) to calculate a single average residual\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspi_model<- \nspi_indicators %>%\n  add_predictions(added_indicators_to_infrastructure[[2]]) %>%                  # Add predictions and residuals\n  add_residuals(added_indicators_to_infrastructure[[2]]) %>%\n  filter(year >= 2016)                              # Remove years without full indicator data\n\nspi_model_summary <-\n  spi_model %>%\n  group_by(country) %>%\n  summarize(\n    income = first(income),\n    region = first(region),\n    mean_resid = mean(resid, na.rm = T),\n    mean_abs_resid = mean(abs(resid), na.rm = T),\n    n_years = sum(!is.na(resid)),                   # How many years of data\n    sse = sum(resid^2, na.rm = T),                  # Sum of squared errors\n    mse = mean(resid^2, na.rm = T)                  # Mean squared error\n  ) %>%\n  filter(!is.na(mean_resid)) %>%                    # Remove countries with NA residuals\n  mutate(\n    income = if_else(country==\"Venezuela, RB\",      # Recode Venezuela\n                     \"Lower middle income\", \n                     income))\n```\n:::\n\n\nThe plot below shows countries relative to their predicted Data Development score. It is noticeable that the Upper Middle income countries are largely outperforming their model prediction, whereas high and low income countries are under performing their prediction. This is most noticeable among low income countries. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nspi_resid_sd <- spi_model_summary$mean_resid %>% sd # Calculate 1 standard deviation of residuals\n  \nggplot(data = spi_model_summary,\n         aes(x = mean_resid, y = fct_reorder(country, mean_resid))) +\n  geom_vline(xintercept = 0, color = \"lightblue\", size = 1) + \n  geom_vline(xintercept = 0+spi_resid_sd, color = \"lightgreen\", size = 0.8) + \n  geom_vline(xintercept = 0-spi_resid_sd, color = \"salmon\", size = 0.8) + \n  geom_point() +\n  facet_wrap(~income, scales =\"free_y\") +\n  labs(title = \"Average residual between the observed Data Development score and the model's predicted score\",\n       subtitle = \"The model predict Data Development score by using the Data infrastructure and Services indicators only \\nPositive residuals indicates that real-world scores were higher than the predicted model.\",\n       x = \"Average Residual between Real-world performance and model\") +\n  theme(axis.title.y = element_blank(),\n        axis.text.y = element_text(size = 3)) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](spi_indicators_regression_files/figure-html/plotting-1.png){width=672}\n:::\n:::\n\n\n#### What indicators improve on the original model for low income countries?\n\nThis prompted a second question. Can low income countries be better modeled using different indicators? There may be other factors in low income countries that makes one indicator or another more representative of its overall Data Development score. For example, maybe a lack of infrastructure development in low income countries made this indicator less predictive of overall development. \n\nI decided to answer this be conducting the same process as before. I filtered the data set to only include low income countries, and then used Forward Selection to arrive at the most predictive indicators. \n\nThe output table below demonstrates that, in fact, data products are the most predictive single indicator. Products were previously the weakest indicator, and refers to the actual statistics which are available for the country (centered on statistics pertaining to Sustainable Development Goals). \n\n\n::: {.cell}\n\n```{.r .cell-code}\nspi_low <-\n  spi_indicators %>%\n  filter(income == \"Low income\")\n\nsingle_low_income_indicators <- map(indicators, ~lm(reformulate(.x, \"overall_score\"), data = spi_low))\n\nmap_dfr(single_low_income_indicators, glance, .id = \"predictor\") %>% arrange(desc(r.squared)) %>%\n  mutate(predictor = factor(predictor, levels = 1:5, labels = str_remove(indicators, \"_score\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 13\n  predictor  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC\n  <fct>          <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl>\n1 data_prod…     0.763         0.762  6.20      583. 1.73e-58     1  -593. 1191.\n2 data_use       0.702         0.701  6.95      427. 1.74e-49     1  -613. 1233.\n3 data_sour…     0.630         0.628  7.75      308. 6.52e-41     1  -633. 1273.\n4 data_serv…     0.612         0.609  7.94      285. 5.19e-39     1  -638. 1281.\n5 data_infr…     0.600         0.598  8.05      272. 6.67e-38     1  -640. 1287.\n# ℹ 4 more variables: BIC <dbl>, deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n:::\n\n\nBoth Data Use and Data Sources were shown to be similarly effective secondary predictors, when added to the primary Data Products indicator. This model accounted for roughly 88% percent of the variation in the Data Development score. The interaction variable did not improve the accuracy of either model.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nadded_low_income_indicators_to_infrastructure <- map(setdiff(indicators, \"data_products_score\"), ~lm(reformulate(termlabels = c(.x, \"data_products_score\"), \"overall_score\"), data = spi_low))\n\nmap_dfr(added_low_income_indicators_to_infrastructure, glance, .id = \"predictor\") %>% arrange(desc(r.squared)) %>%\n  mutate(predictor = factor(predictor, \n                            levels = 1:4, \n                            labels = str_replace(setdiff(indicators, \"data_products_score\"), \"_score\", \" + products\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 13\n  predictor  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC\n  <fct>          <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl>\n1 data_sour…     0.880         0.878  4.43      658. 1.67e-83     2  -530. 1069.\n2 data_use …     0.874         0.872  4.54      623. 1.22e-81     2  -535. 1078.\n3 data_serv…     0.833         0.832  5.21      450. 9.03e-71     2  -560. 1129.\n4 data_infr…     0.829         0.827  5.29      435. 1.14e-69     2  -563. 1134.\n# ℹ 4 more variables: BIC <dbl>, deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n\n```{.r .cell-code}\nlm_low_income_interaction <- lm(overall_score ~ data_products_score:data_sources_score, data = spi_low)\n\nglance(lm_low_income_interaction) %>% mutate(predictor = \"Data Products:Data Sources interaction model\") %>% relocate(predictor)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 13\n  predictor  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC\n  <chr>          <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl>\n1 Data Prod…     0.762         0.761  6.21      581. 2.18e-58     1  -593. 1191.\n# ℹ 4 more variables: BIC <dbl>, deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n:::\n\n\n#### Visualizing the Accuracy of the new Low Income-trained model\n\nBased on other checks of the Data Use vs Data Sources models (such as running a test of internal consistency using Cronbach's alpha), I decided that Data Sources seems to be the better model. I present the same plot as above, but applying the 'low income' model to all countries. While this improves the fit for low income countries, it is a very poor prediction for all other income groups, predicting far below what those groups scored in the real-world. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nspi_model_prod_sourc_low <-\n  spi_indicators %>%\n  add_predictions(added_low_income_indicators_to_infrastructure[[2]]) %>%\n  add_residuals(added_low_income_indicators_to_infrastructure[[2]]) %>%\n  group_by(country) %>%\n  summarize(\n    income = first(income),\n    region = first(region),\n    mean_resid = mean(resid, na.rm = T),\n    mean_abs_resid = mean(abs(resid), na.rm = T),  # Better measure of \"worse\"\n    n_years = sum(!is.na(resid)),                   # How many years of data\n    sse = sum(resid^2, na.rm = T),                  # Sum of squared errors\n    mse = mean(resid^2, na.rm = T)                  # Mean squared error\n  ) %>%\n  filter(!is.na(mean_resid)) %>%\n  mutate(income = if_else(country==\"Venezuela, RB\", \"Lower middle income\", income))\n\nspi_resid_sd_prod_sourc_low <- spi_model_prod_sourc_low$mean_resid %>% sd\n\nggplot(data = spi_model_prod_sourc_low,\n       aes(x = mean_resid, y = fct_reorder(country, mean_resid))) +\n  geom_vline(xintercept = 0, color = \"lightblue\", size = 1) + \n  geom_vline(xintercept = 0+spi_resid_sd_prod_sourc_low, color = \"lightgreen\", size = 0.8) + \n  geom_vline(xintercept = 0-spi_resid_sd_prod_sourc_low, color = \"salmon\", size = 0.8) + \n  geom_point() +\n  facet_wrap(~income, scales =\"free_y\")+\n  labs(title = \"Low income variation: Average residual between the observed Data Development score and the model's predicted score\",\n       subtitle = \"The model predicts Data Development score by using the Data Products and Sources indicators only, and was trained on low income country data only. \\nPositive residuals indicates that real-world scores were higher than the predicted model.\",\n       x = \"Average Residual between Real-world performance and model\") +\n  theme(axis.title.y = element_blank(),\n        axis.text.y = element_text(size = 3)) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](spi_indicators_regression_files/figure-html/low_income_model_plot-1.png){width=672}\n:::\n:::\n\n\n#### Making sense of the two models\n\nWhat do the two models demonstrate about Data Development? \n\nThe original model shows that, for most countries, we can reasonably predict overall Data Development scores through just the Data Infrastructure and Services indicators. Infrastructure includes hard and soft data systems and data governance, whereas services are tools that allow accessibility to a range of data platforms. Together, they represent openness, structure, sophistication of the data environment.\n\nThe residuals show that the income level of a country changes the predictiveness of those two indicators of overall Data Development. \n- High income countries under performed their prediction as a group\n- Middle income countries over performed their prediction as a group\n\nThis may be explained simply due to ceiling effects for high income countries. As a country nears the maximum score, small gaps can reduce scores, and achieving 100% may become increasingly difficult. For middle income countries with lots of room for improvement, growth is more readily obtained. This may have led to more over performance relative to the model, and suggesting that a non-linear model may be better suited.  \n\nOn the other hand, Data Infrastructure and Services indicators generally *over estimate the Data Development score of low income countries*. Despite governance and accessibility features, other aspects of the data environment seem to result in lower overall scores. For such countries, Data Products (the actual statistics available) and Data Sources (whether data collection mechanisms exist) are the more important predictors. Whether or not more sophisticated structure exists is less important here. Instead, we should look at the hard data and tools available in the country, as this is a better indicator of overall Data Development.\n\n\n\n\n\n\n\n\n",
    "supporting": [
      "spi_indicators_regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}